{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import action_parser\n",
    "from extract_coauthor_raw_logs import jsonl_names\n",
    "import level_2_learning_comparisons\n",
    "\n",
    "import re\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f835f",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Extracting logs from zip file\n",
    "# import zipfile\n",
    "\n",
    "# folder_name = '/content/vibewritingpilot.zip'\n",
    "\n",
    "# with zipfile.ZipFile(folder_name, 'r') as zip_ref:\n",
    "#     zip_ref.extractall('/content/pilot2')  # This will extract to /content/pilot_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e67751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_files_parsed(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all .jsonl files in the given folder and returns a dictionary.\n",
    "\n",
    "    Each key in the dictionary is the file name (without extension),\n",
    "    and its value is a list of parsed JSON objects (not strings).\n",
    "    \"\"\"\n",
    "    jsonl_data = {}\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.jsonl'):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(full_path, 'r', encoding='utf-8') as file:\n",
    "                    lines = file.read().splitlines()\n",
    "                    parsed_lines = [json.loads(line) for line in lines]\n",
    "                    jsonl_data[filename.split('.')[0]] = parsed_lines\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "    return jsonl_data\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"formal_raw_logs\"  # Your folder path\n",
    "data = load_jsonl_files_parsed(folder_path)\n",
    "\n",
    "# Print a preview of one file to confirm the format\n",
    "for file_name, events in data.items():\n",
    "    print(f\"\\nFile: {file_name}, Number of events: {len(events)}\")\n",
    "    if events:\n",
    "        print(\"First event:\")\n",
    "        print(json.dumps(events[0], indent=2))  # Pretty-print first event\n",
    "    break  # Remove this if you want to preview more files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3009226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_data = {}\n",
    "filename = \"formal_raw_logs/6df918d72046461a98d275bf3fac31d0.jsonl\"\n",
    "# Print a preview of one file to confirm the format\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    lines = file.read().splitlines()\n",
    "    parsed_lines = [json.loads(line) for line in lines]\n",
    "    jsonl_data[filename.split('.')[0]] = parsed_lines\n",
    "\n",
    "data = jsonl_data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41807849",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_by_session = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d66325",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in logs_by_session.keys():\n",
    "  logs_by_session[key] = logs_by_session[key][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb1b85",
   "metadata": {},
   "source": [
    "### Loading Structured JSON File for Parsing and Analysis\n",
    "\n",
    "Once the `extract_coauthor_raw_logs.py` script has successfully generated a structured JSON file, you can proceed to load the file in this section for further parsing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af38e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current working directory\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Get the raw log dataset\n",
    "file_path = os.path.join(script_dir, 'formal1_logs.json')\n",
    "\n",
    "# Feel free to uncomment the line below and start with a smaller sample (20 writing sessions) to reduce runtime\n",
    "# file_path = os.path.join(script_dir, 'small_logs_for_test.json')\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path) as f:\n",
    "    logs_by_session = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e211d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Load raw JSON-like data from file\n",
    "file_path = os.path.join(script_dir, '/content/testing_new_site.json')\n",
    "with open(file_path, 'r') as f:\n",
    "    raw_data = f.read()\n",
    "\"\"\"\n",
    "\n",
    "raw_data = logs_by_session\n",
    "\n",
    "\n",
    "def fix_all_arrays(raw_json_string):\n",
    "    def fix_array(match):\n",
    "        array_content = match.group(1)\n",
    "        # Add commas between objects: }{\n",
    "        fixed = re.sub(r'\\}\\s*\\{', '},\\n{', array_content.strip())\n",
    "        return f'[{fixed}]'\n",
    "\n",
    "    # Regex: match any array content following a key\n",
    "    fixed_json_string = re.sub(r'\\[\\s*({.*?})\\s*\\]', lambda m: fix_array(m), raw_json_string, flags=re.DOTALL)\n",
    "    return fixed_json_string\n",
    "\n",
    "\n",
    "# This regex looks for a pattern where a closing brace is immediately followed (with any whitespace) by an opening brace,\n",
    "# and inserts a comma between them.\n",
    "raw_data_str = json.dumps(logs_by_session)\n",
    "fixed_data = re.sub(r'(\\})\\s*(\\{)', r'\\1, \\2', raw_data_str)\n",
    "\n",
    "\n",
    "# Now try to load the fixed data as JSON.\n",
    "try:\n",
    "    logs_by_session = json.loads(fixed_data)\n",
    "    print(\"✅ Fixed and loaded JSON successfully!\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"❌ Failed to parse JSON:\", e)\n",
    "\n",
    "\n",
    "for test_user, events in logs_by_session.items():\n",
    "    if isinstance(events, list):\n",
    "       logs_by_session[test_user] = [\n",
    "            event for event in events if event.get(\"eventName\") != \"system-initialize\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea70a40d",
   "metadata": {},
   "source": [
    "### Parsing Raw Log JSON File into Structured Level 1 Actions\n",
    "\n",
    "This section processes raw logs and converts them into Level 1 actions using a analyzer. Each parsed action is enriched with a **level_1_action_type** key, which specifies the action type (e.g., `insert_text`, `delete_text`, `accept_suggestion`).\n",
    "\n",
    "**level_1_actions_per_session** is a dictionary where each session key maps to a list of parsed actions, organizing the output by session for streamlined analysis and further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_strings_similar_lev(str1, str2, max_differences=4):\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    return distance <= max_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current working directory\n",
    "script_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "91733f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_sentences = []\n",
    "\n",
    "\n",
    "def split_insert_text_by_delta(action, prev_action, threshold=5):\n",
    "    \"\"\"\n",
    "    Splits an 'insert_text' action into AI and human based on insert length from delta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ops = action[\"action_logs\"][0][\"textDelta\"][\"ops\"]\n",
    "        inserted_text = ops[1].get(\"insert\", \"\")\n",
    "    except (IndexError, KeyError, TypeError):\n",
    "        action[\"level_1_action_type\"] = \"insert_text_human\"\n",
    "        action[\"human_sentences_temporal_order\"] = \" \".join(\n",
    "            action[\"sentences_temporal_order\"]\n",
    "        )\n",
    "        return [action]\n",
    "\n",
    "    if prev_action[\"level_1_action_type\"] == \"present_suggestion\" and len(inserted_text.strip()) >= threshold:\n",
    "        # ---- AI action ----\n",
    "        ai_action = action.copy()\n",
    "        ai_action[\"action_logs\"] = ai_action[\"action_logs\"][0]\n",
    "        ai_action[\"action_delta\"] = [\n",
    "            \"INSERT\",\n",
    "            inserted_text,\n",
    "            action[\"action_delta\"][2],\n",
    "            action[\"action_delta\"][3],\n",
    "        ]\n",
    "        ai_action[\"action_modified_sentences\"] = utils.sent_tokenize(inserted_text)\n",
    "        ai_sentences.extend(ai_action[\"action_modified_sentences\"])\n",
    "        ai_action[\"action_end_writing\"] = (\n",
    "            ai_action[\"action_start_writing\"] + inserted_text\n",
    "        )\n",
    "        ai_action[\"level_1_action_type\"] = \"insert_text_ai\"\n",
    "\n",
    "        # ---- Human action ----\n",
    "        action_human = action.copy()\n",
    "        action_human[\"action_start_writing\"] = ai_action[\"action_end_writing\"]\n",
    "        remaining_text = action_human[\"action_end_writing\"][\n",
    "            len(action_human[\"action_start_writing\"]) :\n",
    "        ]\n",
    "        action_human[\"action_delta\"] = [\n",
    "            \"INSERT\",\n",
    "            remaining_text,\n",
    "            action[\"action_delta\"][2],\n",
    "            action[\"action_delta\"][3],\n",
    "        ]\n",
    "        action_human[\"action_logs\"] = action_human[\"action_logs\"][1:]\n",
    "        action_human[\"level_1_action_type\"] = \"insert_text_human\"\n",
    "\n",
    "        # ---- Filter modified sentences ----\n",
    "        action_human[\"action_modified_sentences\"] = [\n",
    "            s\n",
    "            for s in action_human[\"action_modified_sentences\"]\n",
    "            if not any(\n",
    "                are_strings_similar_lev(s, ai_s)\n",
    "                for ai_s in ai_action[\"action_modified_sentences\"]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # ---- Filter temporal order ----\n",
    "        sentences_human = [\n",
    "            s\n",
    "            for s in action_human[\"sentences_temporal_order\"]\n",
    "            if s.count(\"$\") < 2 and not any(\n",
    "                are_strings_similar_lev(s, ai_s)\n",
    "                for ai_s in ai_action[\"action_modified_sentences\"]\n",
    "            )\n",
    "        ]\n",
    "        action_human[\"human_sentences_temporal_order\"] = \" \".join(sentences_human)\n",
    "\n",
    "        sentences_without_prompts = [\n",
    "            s\n",
    "            for s in action_human[\"sentences_temporal_order\"]\n",
    "            if (\n",
    "                s.count(\"$\") < 2\n",
    "            )\n",
    "        ]\n",
    "        action_human[\"sentences_temporal_order_without_prompts\"] = sentences_without_prompts\n",
    "        ai_action[\"sentences_temporal_order_without_prompts\"] = sentences_without_prompts\n",
    "        \n",
    "        return [ai_action, action_human]\n",
    "\n",
    "    else:\n",
    "        # ---- Purely human insert ----\n",
    "        action_human = action.copy()\n",
    "        action_human[\"level_1_action_type\"] = \"insert_text_human\"\n",
    "\n",
    "        sentences_human = [\n",
    "            s\n",
    "            for s in action_human[\"sentences_temporal_order\"]\n",
    "            if not any(are_strings_similar_lev(s, ai_s) for ai_s in ai_sentences)\n",
    "        ]\n",
    "        action_human[\"human_sentences_temporal_order\"] = \" \".join(sentences_human)\n",
    "\n",
    "        sentences_without_prompts = [\n",
    "            s for s in action_human[\"sentences_temporal_order\"] if (s.count(\"$\") < 2)\n",
    "        ]\n",
    "        action_human[\"sentences_temporal_order_without_prompts\"] = (\n",
    "            sentences_without_prompts\n",
    "        )\n",
    "\n",
    "        return [action_human]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f04712c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Level 1 Actions:  21%|██▏       | 3/14 [00:00<00:00, 23.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752768728425}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752768728425}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752783091396}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752783091396}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752797464057}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752797464057}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752862294085}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752862294085}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753050258907}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753050258907}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753211326951}\n",
      "NEXT_CLICKED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Level 1 Actions:  57%|█████▋    | 8/14 [00:00<00:00, 16.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753211326951}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752775741979}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752775741979}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752855918299}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1752855918299}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753017123863}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753017123863}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753060358599}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753060358599}\n",
      "NEXT_CLICKED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Level 1 Actions: 100%|██████████| 14/14 [00:00<00:00, 18.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753112478228}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753112478228}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753913882361}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753913882361}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753972753614}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1753972753614}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1754059083586}\n",
      "NEXT_CLICKED\n",
      "Error: {'eventName': 'NEXT_CLICKED', 'eventSource': 'user', 'eventTimestamp': 1754059083586}\n",
      "NEXT_CLICKED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store parsed actions\n",
    "level_1_actions_per_session = {}\n",
    "\n",
    "# Iterate through all sessions in the raw logs and parse actions\n",
    "for session in tqdm(logs_by_session, desc=\"Parsing Level 1 Actions\"):\n",
    "\n",
    "    # Initialize the MergeActionsAnalyzer for each session\n",
    "    actions_analyzer = action_parser.MergeActionsAnalyzer(\n",
    "        last_action=None,\n",
    "        raw_logs=logs_by_session[session]\n",
    "    )\n",
    "\n",
    "    # Parse the logs for the session into structured actions\n",
    "    actions_lst, last_action = actions_analyzer.parse_actions_from_logs(\n",
    "        all_logs=logs_by_session[session],\n",
    "        last_action=None,\n",
    "        DLT_CHAR_MAX_COUNT=9  # Optional: Specify tiny delete threshold here\n",
    "    )\n",
    "\n",
    "    # Store the parsed actions in the output dictionary\n",
    "    level_1_actions_per_session[session] = actions_lst\n",
    "\n",
    "# Add a new key to each action for classification and further analysis\n",
    "for session_key, actions in level_1_actions_per_session.items():\n",
    "    i = 0\n",
    "\n",
    "    while i < len(actions):\n",
    "        action = actions[i]\n",
    "        if i>0 and \"action_type\" in action and action[\"action_type\"] == \"insert_text\":\n",
    "            split_actions = split_insert_text_by_delta(action, actions[i-1])\n",
    "            if len(split_actions) == 1:\n",
    "                actions[i] = split_actions[0]\n",
    "                i += 1\n",
    "            else:\n",
    "                actions[i:i+1] = split_actions\n",
    "                i += len(split_actions)\n",
    "        else:\n",
    "            if not \"action_type\" in action:\n",
    "                action[\"level_1_action_type\"] = \"NEXT_CLICKED\"\n",
    "            else:\n",
    "                action[\"level_1_action_type\"] = action[\"action_type\"]\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985ceb6",
   "metadata": {},
   "source": [
    "Making Human AI Edit Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a823194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "import copy\n",
    "def most_similar_sentence(insert_text, sentences): \n",
    "    \"\"\"Return the most similar sentence and its similarity score.\"\"\" \n",
    "    best_sent, best_score = None, 0 \n",
    "    for s in sentences: \n",
    "        score = SequenceMatcher(None, insert_text.split(), s.split()).ratio() \n",
    "        if score > best_score: \n",
    "            best_sent, best_score = s, score \n",
    "    return best_sent, best_score \n",
    "def get_inserted(before: str, after: str): \n",
    "    \"\"\" Returns the parts of after that were newly inserted compared to before. \"\"\" \n",
    "    sm = SequenceMatcher(None, before.split(), after.split()) \n",
    "    inserted = [] \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes(): \n",
    "        if tag == \"insert\": \n",
    "            inserted.extend(after.split()[j1:j2]) \n",
    "        elif tag == \"replace\": \n",
    "            # Replacement can be seen as deletion + insertion \n",
    "            inserted.extend(after.split()[j1:j2]) \n",
    "    return \" \".join(inserted) \n",
    "def get_deleted(before: str, after: str): \n",
    "    \"\"\" Returns the parts of before that were deleted when transforming into after. \"\"\" \n",
    "    sm = SequenceMatcher(None, before.split(), after.split()) \n",
    "    deleted = [] \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes(): \n",
    "        if tag == \"delete\": \n",
    "            deleted.extend(before.split()[i1:i2]) \n",
    "        elif tag == \"replace\": \n",
    "            # Replacement can be seen as deletion + insertion \n",
    "            deleted.extend(before.split()[i1:i2]) \n",
    "    return \" \".join(deleted) \n",
    "def split_action_delta(action_delta: str, delta: str, min_ratio=0.6): \n",
    "    \"\"\" Try to locate delta inside action_delta, even if noisy like 'For examp c'. Returns (before, match, after). \"\"\" \n",
    "    best = None \n",
    "    best_ratio = 0.0 \n",
    "    # Slide over possible substrings of action_delta \n",
    "    for start in range(len(action_delta)): \n",
    "        for end in range(start+1, min(len(action_delta), start+len(delta)+5)+1): \n",
    "            sub = action_delta[start:end] \n",
    "            ratio = SequenceMatcher(None, sub, delta).ratio() \n",
    "            if ratio > best_ratio: \n",
    "                best_ratio = ratio \n",
    "                best = (start, end, sub) \n",
    "    if best and best_ratio >= min_ratio: \n",
    "        s, e, sub = best \n",
    "        return action_delta[:s], sub, action_delta[e:] \n",
    "    else: # fallback: couldn't match well \n",
    "        return action_delta, \"\", \"\"\n",
    "    \n",
    "def make_end_writing_insert(start, modified_sentences, end):\n",
    "    start_sents = sent_tokenize(start)\n",
    "    end_sents = sent_tokenize(end)\n",
    "    result_sents = []\n",
    "\n",
    "    modified_sentence_idx = 0\n",
    "    \n",
    "    for i in range(max(len(start_sents), len(end_sents))):\n",
    "        if i < len(start_sents):\n",
    "            if start_sents[i] not in end_sents:\n",
    "                result_sents.append(modified_sentences[modified_sentence_idx])\n",
    "                modified_sentence_idx+=1\n",
    "            else:\n",
    "                result_sents.append(start_sents[i])\n",
    "        else:\n",
    "            if modified_sentence_idx != len(modified_sentences):\n",
    "                result_sents+=modified_sentences[modified_sentence_idx:]\n",
    "            break\n",
    "        if modified_sentence_idx == len(modified_sentences):\n",
    "            if i < len(start_sents):\n",
    "                result_sents+=start_sents[i+1:]\n",
    "            break\n",
    "    return \" \".join(result_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a03ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_human_ai_edit(level_1_actions_per_session):\n",
    "    \"\"\"\n",
    "    Parses Level 2 present_suggestion actions based on Level 2 actions.\n",
    "\n",
    "    Args:\n",
    "        level_2_actions_per_session (dict): A dictionary where each session key maps to a list of level 2 actions from insert_text actions\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated actions dictionary with Level 2 attributes added to each action.\n",
    "    \"\"\"\n",
    "    for session_id, actions_lst in tqdm(\n",
    "        level_1_actions_per_session.items(), desc=\"Parsing Level 1 present_suggestion Actions\"\n",
    "    ):\n",
    "        \n",
    "        last_insert_action = \"\"\n",
    "        past_arguments = []\n",
    "        past_prompts = []\n",
    "        past_suggestions = []\n",
    "        for idx, action in enumerate(actions_lst):\n",
    "            try:\n",
    "                if len(action[\"action_delta\"]) > 1:\n",
    "                    if action[\"action_delta\"][0] == 'INSERT':\n",
    "                        # This will compute a word level delta insert\n",
    "                        delta = get_inserted(action[\"action_start_writing\"], action[\"action_end_writing\"])\n",
    "                        action[\"action_delta_by_word\"] = ('INSERT', delta, len(delta), len(delta.split()))\n",
    "                    elif action[\"action_delta\"][0] == 'DELETE':\n",
    "                        delta = get_deleted(action[\"action_start_writing\"], action[\"action_end_writing\"])\n",
    "                        action[\"action_delta_by_word\"] = ('DELETE', delta, len(delta), len(delta.split()))\n",
    "                for suggestion in past_suggestions: \n",
    "                    # Does not account for movements, just check if the same string is still in the writing after current action\n",
    "                    if suggestion in action[\"action_start_writing\"] and suggestion not in action[\"action_end_writing\"]:\n",
    "                        if action[\"action_delta\"][0] == \"INSERT\":\n",
    "                            #print(suggestion)\n",
    "                            sent,score = most_similar_sentence(suggestion, action[\"action_modified_sentences\"])\n",
    "                            if sent is None or len(sent.split()) <= 2:\n",
    "                                continue\n",
    "                            #sent_idx = action[\"action_modified_sentences\"].index(sent)\n",
    "                            delta = get_inserted(suggestion, sent)\n",
    "                            # Making end writing on a sentence level, assume linear writing within a block (might not be true but good enough proxy?)\n",
    "                            # Splits the action modified sentences by before human edit ai, human edit ai sentence, and after human edit ai \n",
    "                            # Replace start writing with every modified sentence before human edit ai to be start writing for mid and repeat\n",
    "                            if len(action[\"action_modified_sentences\"]) > 1:\n",
    "                                action_mid = copy.deepcopy(action)\n",
    "                                action_mid[\"level_2_action_type\"] = \"human_edit_ai\"\n",
    "                                action_mid[\"action_delta_by_word\"] = (\"INSERT\", delta, len(delta), len(delta.split()))\n",
    "                                temporal_modified_sentences = []\n",
    "                                for s in action[\"sentences_temporal_order\"]:\n",
    "                                    if s in action[\"action_modified_sentences\"]:\n",
    "                                        temporal_modified_sentences.append(s)\n",
    "                                sent_idx = temporal_modified_sentences.index(sent)\n",
    "                                #after = action[\"action_modified_sentences\"][sent_idx:]\n",
    "                                #before = action[\"action_modified_sentences\"][:sent_idx]\n",
    "                                after = temporal_modified_sentences[sent_idx:]\n",
    "                                before = temporal_modified_sentences[:sent_idx]\n",
    "                                #print(\"before\")\n",
    "                                #print(before)\n",
    "                                #print(after[1:])\n",
    "                                if action in actions_lst:\n",
    "                                    actions_lst.remove(action)\n",
    "                                if len(before) > 0:\n",
    "                                    action_before = copy.deepcopy(action)\n",
    "                                    action_before[\"action_modified_sentences\"] = before\n",
    "                                    action_before[\"action_end_writing\"] = make_end_writing_insert(action_before[\"action_start_writing\"], action_before[\"action_modified_sentences\"], action_before[\"action_end_writing\"])\n",
    "                                    action_mid[\"action_start_writing\"] = action_before[\"action_end_writing\"]\n",
    "                                    actions_lst.insert(idx, action_before)\n",
    "                                    idx+=1\n",
    "                                action_mid[\"action_modified_sentences\"] = [sent]#action[\"action_modified_sentences\"][sent_idx]\n",
    "                                action_mid[\"action_end_writing\"] = make_end_writing_insert(action_mid[\"action_start_writing\"], action_mid[\"action_modified_sentences\"], action_mid[\"action_end_writing\"])\n",
    "                                actions_lst.insert(idx, action_mid)\n",
    "                                idx+=1\n",
    "                                if len(after) > 1: \n",
    "                                    action_after = copy.deepcopy(action)\n",
    "                                    action_after[\"action_modified_sentences\"] = after[1:]\n",
    "                                    action_after[\"action_start_writing\"] = action_mid[\"action_end_writing\"]\n",
    "                                    actions_lst.insert(idx, action_after)\n",
    "                                    idx+=1\n",
    "                            else:\n",
    "                                #action[\"level_1_action_type\"] = \"human_edit_ai_insert\"\n",
    "                                action[\"level_2_action_type\"] = \"human_edit_ai\"\n",
    "                            \n",
    "                        # Kind of just colored the whole block as human edit ai delete since people usually delete in whole block so doesn't make as much sense to split. \n",
    "                        elif action[\"action_delta\"][0] == 'DELETE':\n",
    "                            action[\"level_2_action_type\"] = \"human_edit_ai\"\n",
    "                        past_suggestions.remove(suggestion)\n",
    "                if action[\"level_1_action_type\"] == \"insert_text_ai\":\n",
    "                    past_suggestions.append(action[\"action_modified_sentences\"][0])\n",
    "                \n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "\n",
    "                print(f\"\\n!! Error in session {session_id}, action index {idx} !!\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "    return level_1_actions_per_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6ff59c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Level 1 present_suggestion Actions: 100%|██████████| 14/14 [00:00<00:00, 78.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "['While corporate personhood has controversial implications within the sphere of campaign finance, it has also been applied to other areas of corporations.']\n",
      "['This can change the type of court cases that corporations are affected by.', 'Typically, corporations are mostly affected by consumer protection cases (individual vs.', 'corporation), but if corporate personhood allows for corporations to be sued as individual vs.', 'individual, then it can fall into']\n",
      "before\n",
      "[]\n",
      "['$example of corporate personhood$']\n",
      "before\n",
      "['$other benefits of corporate personhood$']\n",
      "[]\n",
      "before\n",
      "['In this sense, people believed that Whether that was having the freedom to fund the candidate that they desire to or express political opinions without fear of government retaliation.']\n",
      "[]\n",
      "before\n",
      "['These zero-price markets have increased in variety and number, with examples such as social media, travel booking, or software.']\n",
      "['$add two sentences about google lawsuit about dominating google search engine$']\n",
      "before\n",
      "[]\n",
      "['$example of stricter data privacy law in zero price market$']\n",
      "before\n",
      "[]\n",
      "['$was the GDPR in europe successful?$']\n",
      "before\n",
      "['The GDPR improved awareness about privacy rights but faced challenges in enforcement and compliance from companies.']\n",
      "[\"$can you make me a full fleshed argument why we don't need antitrust regulation in zero-price markets$\"]\n",
      "before\n",
      "[]\n",
      "[\"In contrast, some argue that since consumers aren't paying, antitrust regulation is unnecessary in zero-price markets.\", \"$can you make me a full fleshed argument why we don't need antitrust regulation in zero-price markets$\"]\n",
      "before\n",
      "['Without money involved in the operation, data is used as the new \"currency\" in zero-price markets.']\n",
      "[]\n",
      "before\n",
      "['Furthermore, users often choose convenience over privacy, shaping how companies operate in these markets.']\n",
      "[]\n",
      "before\n",
      "[\"$why don't traditinal measures of h$\"]\n",
      "[]\n",
      "before\n",
      "[]\n",
      "['Conversely, some believe that new regulation is not necessary, since these markets often rely']\n",
      "before\n",
      "[]\n",
      "['This is clearly a need for an alternitve way tTracking user data as a form of payment could help gauge value and protect consumer interests.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "level_1_actions_per_session_suggestions = split_human_ai_edit(\n",
    "      level_1_actions_per_session\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "48b1796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path in the same directory as this notebook\n",
    "output_file = os.path.join(script_dir, \"level_1_actions_per_session.json\")\n",
    "\n",
    "# Write the parsed actions dictionary to the JSON file\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(level_1_actions_per_session, f, default=utils.custom_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69164303",
   "metadata": {},
   "source": [
    "### Level 2 Learning Parsing Based on Generated Level 1 Actions and Outcome Measures Documentation\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "The pipeline consists of two main steps:\n",
    "\n",
    "1. **Level 2 Parsing**: Augments Level 1 actions with detailed Level 2 attributes.\n",
    "    <br/>Attributes include:\n",
    "- vibe_writing: prewriting opinions are opposed with genAI writing task\n",
    "- minor_vibe_writing: prewriting opinions are potentially opposed with genAI writing task\n",
    "- constructive_learning: applied prewriting learning to genAI writing task (over 3/4 of the writing was mentioned before in the prewriting)\n",
    "- minor_constructive_learning: applied prewriting learning to some of genAI writing task (over 1/4 of the writing was mentioned before in the prewriting)\n",
    "    \n",
    "2. **Outcome Measures Computation**: Computes Stance Detection and Natural Language Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79808a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrontstageText(action, past_prompts_with_ai_inserts):\n",
    "    frontstage_text = action[\"sentences_temporal_order\"].copy()\n",
    "    for sentIndex in range(len(action[\"sentences_temporal_order\"])):\n",
    "        if action[\"sentences_temporal_order\"][sentIndex].count(\"$\")>=2:\n",
    "            index = frontstage_text.index(action[\"sentences_temporal_order\"][sentIndex])\n",
    "            deleted_ai_prompt = frontstage_text.pop(index)\n",
    "            if (\n",
    "                index < len(frontstage_text)\n",
    "                and deleted_ai_prompt in past_prompts_with_ai_inserts\n",
    "                and past_prompts_with_ai_inserts[deleted_ai_prompt]\n",
    "                == frontstage_text[index]\n",
    "            ):\n",
    "                frontstage_text.pop(index)\n",
    "    return frontstage_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8124f1c",
   "metadata": {},
   "source": [
    "Level 2 Action Types: \n",
    "\n",
    "**insert_human_backstage** -- A: Human adds texts to the backstage\n",
    "\n",
    "**insert_ai_backstage** -- B: AI adds texts to the backstage\n",
    "* AI following a prompt\n",
    "* AI suggestions shown\n",
    "\n",
    "**move_frontstage** -- C: Human elevates AI texts from backstage to front stage\n",
    "* moves or copies AI responses to the writing (not implemented) \n",
    "* Accept inline \n",
    "* Deleting prompt to make suggestion frontstage \n",
    "\n",
    "**human_edit_ai** -- D: Humans edit AI texts \n",
    "\n",
    "**insert_human_frontstage** -- E: Human adds texts to the front stage. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48719080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_level_2_actions(level_1_actions_per_session):\n",
    "    \"\"\"\n",
    "    Parses Level 2 actions based on Level 1 actions, but only for insert_text actions.\n",
    "\n",
    "    Args:\n",
    "        level_1_actions_per_session (dict): A dictionary where each session key maps to a list of level 1 actions.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated actions dictionary with Level 2 attributes added to each action.\n",
    "    \"\"\"\n",
    "\n",
    "    for session_id, actions_lst in tqdm(\n",
    "        level_1_actions_per_session.items(), desc=\"Parsing Level 2 Actions\"\n",
    "    ):\n",
    "        past_ai_suggestions = []\n",
    "        past_prompts = []\n",
    "        past_prompts_with_ai_inserts = {}\n",
    "        frontstage_text = []\n",
    "        backstage_text = []\n",
    "        prewriting = False\n",
    "\n",
    "        prewriting_ids = {\n",
    "            \"legislation_corporate_1\": \"ab7b7355bfde4657b68f7a2985e494a9\",\n",
    "            \"legislation_antitrust_1\": \"4d85d96c21494339bfe570b87f354fec\",\n",
    "            \"legislation_corporate_2\": \"4517db08add74465b0888b3002170e59\",\n",
    "            \"legislation_corporate_3\": \"769f457ab1f2458abf376c4d248ae34e\",\n",
    "            \"legislation_antitrust_2\": \"7204e89b27a744c3a256197c77b33f19\",\n",
    "            \"legislation_corporate_4\": \"8e633d2f2e9246828ab498f5388559bc\",\n",
    "            \"legislation_antitrust_3\": \"039142481529449685e10dd7d2a250d6\",\n",
    "            \"legislation_corporate_5\": \"81d7a2e4bdf94cb3a2376afc93410a10\",\n",
    "            \"legislation_antitrust_4\": \"4eedf1ef4ea14d769e65d657f4145f0c\",\n",
    "            \"legislation_antitrust_5\": \"9f742fd07a35428eacee633b2a284120\",\n",
    "            \"legislation_corporate_6\": \"ada424f7f72e46c18c906f14ac4b0e2e\",\n",
    "            \"legislation_antitrust_6\": \"e626c3e15f9641ef94298979e83f9bed\",\n",
    "            \"legislation_antitrust_7\": \"9c861b5263f24c33832ae6dee287cb42\",\n",
    "            \"legislation_antitrust_8\": \"b6a1710464d047b2aef7979a1f66dd5c\",\n",
    "        }\n",
    "        filename = utils.get_filename(\n",
    "            \"prewriting_content/\", prewriting_ids[session_id]\n",
    "        )\n",
    "        prewriting_content = \"\"\n",
    "\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                if \"content\" in data:\n",
    "                    prewriting_content += data[\"content\"]\n",
    "\n",
    "        for idx, action in enumerate(actions_lst):\n",
    "            try:\n",
    "                if action[\"action_type\"] == None:\n",
    "                    prewriting = True\n",
    "                if prewriting:\n",
    "                    if action[\"action_type\"] == \"present_suggestion\" and action[\"action_logs\"][0][\"eventName\"] == \"suggestion-open\":\n",
    "                        last_ai_suggestions = []\n",
    "                        for suggestion in action[\"action_logs\"][0][\"currentSuggestions\"]:\n",
    "                            last_ai_suggestions.append(suggestion[\"trimmed\"])\n",
    "                        backstage_text.append(\" \".join(last_ai_suggestions))\n",
    "\n",
    "                    # detecting human prompting AI\n",
    "                    if (\n",
    "                        action[\"level_1_action_type\"] == \"insert_text_human\"\n",
    "                        and \"action_start_writing\" in action\n",
    "                        and \"action_end_writing\" in action\n",
    "                    ):\n",
    "                        start_text = action[\"action_start_writing\"]\n",
    "                        end_text = action[\"action_end_writing\"]\n",
    "\n",
    "                        start_prompts = utils.extract_prompts(start_text)\n",
    "                        end_prompts = utils.extract_prompts(end_text)\n",
    "\n",
    "                        for end_prompt in end_prompts:\n",
    "                            if not end_prompt in start_prompts:\n",
    "                                action[\"level_1_action_type\"] = \"insert_text_human_ai_prompt\"\n",
    "                                past_prompts.append(end_prompt)\n",
    "                                action[\"past_ai_prompts\"] = past_prompts.copy()\n",
    "                                backstage_text.append(end_prompt)\n",
    "                                break\n",
    "\n",
    "                    # detecting all human text inserts as frontstage\n",
    "                    if(action[\"level_1_action_type\"] == \"insert_text_human\"):\n",
    "                        action[\"writing_type\"] = \"frontstage\"\n",
    "                        action[\"level_2_action_type\"] = \"insert_human_frontstage\"\n",
    "                        frontstage_text = getFrontstageText(\n",
    "                            action, past_prompts_with_ai_inserts\n",
    "                        )\n",
    "\n",
    "                    # detecting AI inserts after prompts labeled as \"unused\"\n",
    "                    if (action[\"level_1_action_type\"] == \"insert_text_ai\" \n",
    "                        and \"action_start_writing\" in action\n",
    "                        and \"action_end_writing\" in action):\n",
    "                        inserted_text = action[\"action_delta\"][1]\n",
    "                        \n",
    "                        sentIndex = -1\n",
    "                        for index in range(len(action[\"sentences_temporal_order\"])):\n",
    "                            if inserted_text in action[\"sentences_temporal_order\"][index]:\n",
    "                                sentIndex = index\n",
    "                        if sentIndex>0 and action[\"sentences_temporal_order\"][sentIndex-1].count(\"$\")>=2:\n",
    "                            action[\"writing_type\"] = \"backstage\"\n",
    "                            action[\"level_2_action_type\"] = \"insert_ai_backstage\"\n",
    "                            past_prompts_with_ai_inserts[action[\"sentences_temporal_order\"][sentIndex - 1]] = inserted_text\n",
    "                            backstage_text.append(action[\"sentences_temporal_order\"][sentIndex])\n",
    "                        else:\n",
    "                            action[\"writing_type\"] = \"frontstage\"\n",
    "                            action[\"level_2_action_type\"] = \"move_frontstage\"\n",
    "                            frontstage_text = getFrontstageText(\n",
    "                                action, past_prompts_with_ai_inserts\n",
    "                            )\n",
    "\n",
    "                    # detecting deletes of prompts to make AI inserted suggestions as frontstage\n",
    "                    if (action[\"level_1_action_type\"] == \"delete_text\"):\n",
    "                        action[\"writing_type\"] = \"frontstage\"\n",
    "                        frontstage_text = getFrontstageText(\n",
    "                                action, past_prompts_with_ai_inserts\n",
    "                            )\n",
    "                        if action[\"action_delta\"][1].count(\"$\")>=2:\n",
    "                            if (\n",
    "                                action[\"action_delta\"][1]\n",
    "                                in past_prompts_with_ai_inserts\n",
    "                                and past_prompts_with_ai_inserts[action[\"action_delta\"][1]]\n",
    "                                in backstage_text\n",
    "                            ):\n",
    "                                action[\"writing_type\"] = \"frontstage\"\n",
    "                                action[\"level_2_action_type\"] = \"move_frontstage\"\n",
    "                                backstage_text.remove(\n",
    "                                    past_prompts_with_ai_inserts[\n",
    "                                        action[\"action_delta\"][1]\n",
    "                                    ]\n",
    "                                )\n",
    "                    \n",
    "                    if (action[\"level_1_action_type\"] == \"insert_text_human_ai_prompt\"):\n",
    "                        action[\"level_2_action_type\"] = \"insert_human_backstage\"\n",
    "                    \n",
    "                    if (action[\"action_type\"] == \"present_suggestion\" and action[\"action_logs\"][0][\"eventName\"] == \"suggestion-open\"):\n",
    "                        action[\"level_2_action_type\"] = \"insert_ai_backstage\"\n",
    "\n",
    "                    action[\"frontstage_text\"] = frontstage_text.copy()\n",
    "                    action[\"backstage_text\"] = backstage_text.copy()\n",
    "                    # differentiating backstage and frontstage writing\n",
    "                    # 1. Backstage writing: human written prompts for AI between 2 $s, inserted AI suggestions right after a prompt, and AI generated suggestions\n",
    "                    if(action[\"level_1_action_type\"] == \"insert_text_human_ai_prompt\" or (action[\"action_type\"] == \"present_suggestion\" and action[\"action_logs\"][0][\"eventName\"] == \"suggestion-open\")):\n",
    "                        action[\"writing_type\"] = \"backstage\"\n",
    "\n",
    "                    # print(action[\"level_1_action_type\"])\n",
    "\n",
    "                    if \"writing_type\" in action:\n",
    "                        print(action[\"level_1_action_type\"])\n",
    "                        print(action[\"writing_type\"])\n",
    "                        print(\"frontstage: \", action[\"frontstage_text\"])\n",
    "                        print(\"backstage: \", action[\"backstage_text\"])\n",
    "                else:\n",
    "                    action[\"level_2_action_type\"] = \"insert_human_frontstage\"\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "\n",
    "                print(f\"\\n!! Error in session {session_id}, action index {idx} !!\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "    return level_1_actions_per_session\n",
    "\n",
    "\n",
    "# Parse Level 2 actions\n",
    "with open(\"level_1_actions_per_session.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    level_1_actions_per_session = json.load(f)\n",
    "\n",
    "level_2_actions_per_session = parse_level_2_actions(\n",
    "    level_1_actions_per_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_level_2_actions = level_2_actions_per_session.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad11352",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_2_actions_per_session = parsed_level_2_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19191358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_level_2_actions_suggestions(level_2_actions_per_session):\n",
    "    \"\"\"\n",
    "    Parses Level 2 present_suggestion actions based on Level 2 actions.\n",
    "\n",
    "    Args:\n",
    "        level_2_actions_per_session (dict): A dictionary where each session key maps to a list of level 2 actions from insert_text actions\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated actions dictionary with Level 2 attributes added to each action.\n",
    "    \"\"\"\n",
    "\n",
    "    for session_id, actions_lst in tqdm(\n",
    "        level_2_actions_per_session.items(), desc=\"Parsing Level 2 present_suggestion Actions\"\n",
    "    ):\n",
    "        last_insert_action = \"\"\n",
    "        past_arguments = []\n",
    "        past_prompts = []\n",
    "        for idx, action in enumerate(actions_lst):\n",
    "            try:\n",
    "                if action[\"action_type\"] and \"insert\" in action[\"level_1_action_type\"]:\n",
    "                    last_insert_action = action[\"level_1_action_type\"]\n",
    "                    if action[\"level_1_action_type\"] == \"insert_text_human_ai_prompt\":\n",
    "                        past_arguments = action.get(\"past_arguments\",[])\n",
    "                        past_prompts = action[\"past_ai_prompts\"]\n",
    "                # ai_suggestions=[]\n",
    "                # if (\n",
    "                #     action[\"action_type\"] == \"present_suggestion\"\n",
    "                #     and action[\"action_logs\"][0][\"eventName\"] == \"suggestion-open\"\n",
    "                # ):\n",
    "                #     for suggestion in action[\"action_logs\"][0][\"currentSuggestions\"]:\n",
    "                #         ai_suggestions.append(suggestion[\"trimmed\"])\n",
    "                #     external_example_dict = level_2_learning_comparisons.get_external_example(\"\".join(ai_suggestions))\n",
    "                #     action[\"external_examples_dict\"] = external_example_dict\n",
    "                #     action[\"external_examples\"] = level_2_learning_comparisons.parse_level_2_external_examples(external_example_dict) * 5\n",
    "                #     continue\n",
    "                if (\n",
    "                    not action[\"action_type\"] or not action[\"action_logs\"] or action.get(\"level_1_action_type\") != \"query_suggestion\"\n",
    "                    # and action[\"action_logs\"][0][\"eventName\"] != \"suggestion-get\"\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                label = \"\"\n",
    "                suggestion_open = action[\"action_logs\"][0]\n",
    "                if suggestion_open[\"currentCursor\"] == len(action[\"action_start_writing\"]) + 1:\n",
    "                    label+=\"end_writing_\"\n",
    "                    stripped_writing = action[\"action_start_writing\"].strip()\n",
    "                    if stripped_writing[-1] in [\".\", \"!\", \"?\"] or action[\"action_start_writing\"][-2:] == \"\\n\":\n",
    "                        label+=\"end_sentence\"\n",
    "                    else:\n",
    "                        label+=\"middle_sentence\"\n",
    "                elif suggestion_open[\"currentCursor\"] == 1:\n",
    "                    label+=\"start_writing\"\n",
    "                else:\n",
    "                    label += \"middle_writing_\"\n",
    "                    writingBefore = action[\"action_start_writing\"][:suggestion_open[\"currentCursor\"]+1]\n",
    "                    writingBefore = writingBefore.strip()\n",
    "                    if writingBefore==\"\":\n",
    "                        label = \"start_writing\"\n",
    "                    elif (\n",
    "                        writingBefore[-1]\n",
    "                        in [\".\", \"!\", \"?\"]\n",
    "                        or writingBefore[-2:] == \"\\n\"\n",
    "                    ):\n",
    "                        label += \"end_sentence\"\n",
    "                    else:\n",
    "                        label += \"middle_sentence\"\n",
    "                action[\"sentence_location\"] = label\n",
    "\n",
    "                # last_suggestion_index = utils.find_last_suggestion(action[\"action_end_writing\"])\n",
    "                # end_suggestion_index = action[\"action_end_writing\"].rfind(\"$\")\n",
    "                # if last_suggestion_index == 0:\n",
    "                #     last_suggestion_index = utils.find_last_punctuation(action[\"action_end_writing\"])\n",
    "                #     end_suggestion_index = len(action[\"action_end_writing\"])\n",
    "\n",
    "                if last_insert_action == \"insert_text_human_ai_prompt\" and past_arguments != []:\n",
    "                    classification = level_2_learning_comparisons.parse_classify_text(\n",
    "                        past_prompts[-1]\n",
    "                    )\n",
    "                    if classification != \"\":\n",
    "                        action[\"text_classification\"] = classification\n",
    "                    past_args = past_arguments.copy()\n",
    "                    if len(past_args) > 4:\n",
    "                        past_args = past_args[-4:]\n",
    "                    past_args = \" \".join(past_args)\n",
    "                    if (len(nli_tokenizer.tokenize(past_args))>450):\n",
    "                        past_args = \" \".join(past_arguments.copy()[-2:])\n",
    "                    \n",
    "                    # nli, nli_info_dct = level_2_learning_comparisons.get_NLI(\n",
    "                    #     past_args, past_prompts[-1]\n",
    "                    # )\n",
    "                    # stance = level_2_learning_comparisons.get_stance_difference(\n",
    "                    #     past_args, past_prompts[-1]\n",
    "                    # )\n",
    "                    # action[\"nli_info\"] = nli_info_dct\n",
    "                    # if stance[\"labels\"][0] == \"disagrees\" and nli == \"contradiction\":\n",
    "                    #     action[\"stance\"] = \"counter prompt\"\n",
    "                    # elif (stance[\"labels\"][0] == \"disagrees\" and nli != \"entailment\") or (nli == \"contradiction\" and stance[\"labels\"][0] != \"agrees\"):\n",
    "                    #     action[\"stance\"] = \"minor counter prompt\"\n",
    "\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "\n",
    "                print(f\"\\n!! Error in session {session_id}, action index {idx} !!\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "    return level_2_actions_per_session\n",
    "\n",
    "# Parse Level 2 present_suggestions actions\n",
    "level_2_actions_per_session_suggestions = parse_level_2_actions_suggestions(\n",
    "    level_2_actions_per_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_level_2_actions_semantic(level_2_actions_per_session_suggestions):\n",
    "    \"\"\"\n",
    "    Parses Level 2 actions based on Level 2 suggestion actions, but only for actions with a writing_type\n",
    "\n",
    "    Args:\n",
    "        level_1_actions_per_session (dict): A dictionary where each session key maps to a list of level 1 actions.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated actions dictionary with Level 2 attributes added to each action.\n",
    "    \"\"\"\n",
    "\n",
    "    for session_id, actions_lst in tqdm(\n",
    "        level_2_actions_per_session_suggestions.items(), desc=\"Parsing Level 2 Actions\"\n",
    "    ):\n",
    "        past_ai_suggestions = []\n",
    "        past_prompts = []\n",
    "        past_prompts_with_ai_inserts = {}\n",
    "        frontstage_text = []\n",
    "        backstage_text = []\n",
    "        prewriting = False\n",
    "\n",
    "        prewriting_ids = {\n",
    "            \"legislation_corporate_1\": \"ab7b7355bfde4657b68f7a2985e494a9\",\n",
    "            \"legislation_antitrust_1\": \"4d85d96c21494339bfe570b87f354fec\",\n",
    "            \"legislation_corporate_2\": \"4517db08add74465b0888b3002170e59\",\n",
    "            \"legislation_corporate_3\": \"769f457ab1f2458abf376c4d248ae34e\",\n",
    "            \"legislation_antitrust_2\": \"7204e89b27a744c3a256197c77b33f19\",\n",
    "            \"legislation_corporate_4\": \"8e633d2f2e9246828ab498f5388559bc\",\n",
    "            \"legislation_antitrust_3\": \"039142481529449685e10dd7d2a250d6\",\n",
    "            \"legislation_corporate_5\": \"81d7a2e4bdf94cb3a2376afc93410a10\",\n",
    "            \"legislation_antitrust_4\": \"4eedf1ef4ea14d769e65d657f4145f0c\",\n",
    "            \"legislation_antitrust_5\": \"9f742fd07a35428eacee633b2a284120\",\n",
    "            \"legislation_corporate_6\": \"ada424f7f72e46c18c906f14ac4b0e2e\",\n",
    "            \"legislation_antitrust_6\": \"e626c3e15f9641ef94298979e83f9bed\",\n",
    "            \"legislation_antitrust_7\": \"9c861b5263f24c33832ae6dee287cb42\",\n",
    "            \"legislation_antitrust_8\": \"b6a1710464d047b2aef7979a1f66dd5c\",\n",
    "        }\n",
    "        filename = utils.get_filename(\"prewriting_content/\", prewriting_ids[session_id])\n",
    "        prewriting_content = \"\"\n",
    "\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                if \"content\" in data:\n",
    "                    prewriting_content += data[\"content\"]\n",
    "\n",
    "        background_info = \"\"\n",
    "        if \"antitrust\" in session_id:\n",
    "            background_info = utils.background_info[0]\n",
    "        else:\n",
    "            background_info = utils.background_info[1]\n",
    "\n",
    "        total_prewriting = background_info + prewriting_content\n",
    "        A_sents = utils.sent_tokenize(total_prewriting)\n",
    "        A_emb = level_2_learning_comparisons.prepare_paragraph_A(A_sents)\n",
    "\n",
    "        last_semantic_expansion_score = 0.0\n",
    "\n",
    "        for idx, action in enumerate(actions_lst):\n",
    "            try:\n",
    "                if \"writing_type\" in action:\n",
    "                    frontstage_text = action[\"frontstage_text\"].copy()\n",
    "                    if action[\"writing_type\"] == \"backstage\":\n",
    "                        backstage_addition = utils.sent_tokenize(\n",
    "                            action[\"backstage_text\"][-1]\n",
    "                        )\n",
    "                        frontstage_text += backstage_addition\n",
    "                    semantic_expansion_df = level_2_learning_comparisons.compute_novelty_paragraphB_vs_A(A_sents, A_emb, frontstage_text)\n",
    "                    if action[\"level_1_action_type\"] == \"present_suggestion\":\n",
    "                        action[\"semantic_expansion\"] = (\n",
    "                            semantic_expansion_df[\"novelty\"].iloc[:-5].sum()\n",
    "                            + semantic_expansion_df[\"novelty\"].iloc[-5:].mean()\n",
    "                            - last_semantic_expansion_score\n",
    "                        )\n",
    "                    else:\n",
    "                        action[\"semantic_expansion\"] = (\n",
    "                            semantic_expansion_df[\"novelty\"].sum()\n",
    "                            - last_semantic_expansion_score\n",
    "                        )\n",
    "\n",
    "                    if \"writing_type\" == \"backstage\":\n",
    "                        if action[\"level_1_action_type\"] == \"present_suggestion\":\n",
    "                            last_semantic_expansion_score = (\n",
    "                                semantic_expansion_df[\"novelty\"].iloc[:-5].sum()\n",
    "                            )\n",
    "                        else:\n",
    "                            last_semantic_expansion_score = (\n",
    "                                semantic_expansion_df[\"novelty\"].iloc[:-1].sum()\n",
    "                            )\n",
    "                    else:\n",
    "                        last_semantic_expansion_score = semantic_expansion_df[\n",
    "                            \"novelty\"\n",
    "                        ].sum()\n",
    "                    \n",
    "                    print(action[\"level_1_action_type\"])\n",
    "                    print(action[\"writing_type\"])\n",
    "                    print(action[\"semantic_expansion\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "\n",
    "                print(f\"\\n!! Error in session {session_id}, action index {idx} !!\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "    return level_1_actions_per_session\n",
    "\n",
    "\n",
    "# Parse Level 2 actions semantic expansion\n",
    "with open(\"level_1_actions_per_session.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    level_1_actions_per_session = json.load(f)\n",
    "\n",
    "level_2_actions_per_session_semantic = parse_level_2_actions_semantic(level_2_actions_per_session_suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_1_actions = [\n",
    "  (idx, action[\"level_1_action_type\"]) for idx, action in enumerate(level_2_actions_per_session[\"legislation_corporate_6\"]) if \"level_1_action_type\" in action\n",
    "]\n",
    "\n",
    "level_2_actions = [\n",
    "  (idx, action[\"level_2_action_type\"]) for idx, action in enumerate(level_2_actions_per_session[\"legislation_corporate_6\"]) if \"level_2_action_type\" in action\n",
    "]\n",
    "\n",
    "only_level1 = [\n",
    "    (idx, action[\"level_1_action_type\"], action[\"action_delta\"])\n",
    "    for idx, action in enumerate(level_2_actions_per_session[\"legislation_corporate_6\"])\n",
    "    if action.get(\"level_1_action_type\") and not action.get(\"writing_type\")\n",
    "]\n",
    "\n",
    "for i,j,k in only_level1:\n",
    "  print(i,j,k)\n",
    "print([i for i in level_2_actions if i[1] == \"human_edit_ai\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current working directory\n",
    "script_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path\n",
    "script_dir = os.getcwd()\n",
    "output_file = os.path.join(script_dir, \"level_2_actions_per_session_semantic.json\")\n",
    "\n",
    "# Save the parsed actions to the JSON file\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(\n",
    "        level_2_actions_per_session_semantic, f, default=utils.custom_serializer\n",
    "    )\n",
    "\n",
    "# Confirm successful save\n",
    "print(f\"Level 2 actions successfully saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
